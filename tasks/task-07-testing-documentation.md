# Task 07: Testing and Final Documentation

**Difficulty:** Intermediate
**Estimated Time:** 4-5 hours
**Prerequisites:** All previous tasks completed

## Goal

Implement comprehensive testing (unit, integration, and end-to-end tests) and complete final documentation for the project.

## Learning Objectives

By completing this task, you will learn:
- How to write unit tests with Jest
- How to write integration tests for APIs
- How to write end-to-end tests for the complete system
- How to measure test coverage
- How to document a production system

## Step-by-Step Instructions

### Step 1: Set Up Testing Infrastructure

```bash
cd apps/orchestrator

# Verify Jest is installed
pnpm list jest @nestjs/testing

# Create test configuration
cat > jest.config.js << 'EOF'
module.exports = {
  moduleFileExtensions: ['js', 'json', 'ts'],
  rootDir: 'src',
  testRegex: '.*\\.spec\\.ts$',
  transform: {
    '^.+\\.(t|j)s$': 'ts-jest',
  },
  collectCoverageFrom: [
    '**/*.(t|j)s',
    '!**/*.spec.ts',
    '!**/node_modules/**',
  ],
  coverageDirectory: '../coverage',
  testEnvironment: 'node',
  moduleNameMapper: {
    '^@shared/(.*)$': '<rootDir>/../../packages/shared/src/$1',
  },
};
EOF
```

### Step 2: Write Unit Tests for Services

```bash
# Tasks Service Tests
cat > src/modules/tasks/tasks.service.spec.ts << 'EOF'
import { Test, TestingModule } from '@nestjs/testing';
import { getRepositoryToken } from '@nestjs/typeorm';
import { Repository } from 'typeorm';
import { NotFoundException } from '@nestjs/common';
import { TasksService } from './tasks.service';
import { TaskEntity } from './entities/task.entity';
import { TaskStatus } from '@shared/types';

describe('TasksService', () => {
  let service: TasksService;
  let repository: Repository<TaskEntity>;

  const mockRepository = {
    create: jest.fn(),
    save: jest.fn(),
    findOne: jest.fn(),
    remove: jest.fn(),
    createQueryBuilder: jest.fn(() => ({
      where: jest.fn().mockReturnThis(),
      orderBy: jest.fn().mockReturnThis(),
      skip: jest.fn().mockReturnThis(),
      take: jest.fn().mockReturnThis(),
      getManyAndCount: jest.fn(),
    })),
    count: jest.fn(),
  };

  beforeEach(async () => {
    const module: TestingModule = await Test.createTestingModule({
      providers: [
        TasksService,
        {
          provide: getRepositoryToken(TaskEntity),
          useValue: mockRepository,
        },
      ],
    }).compile();

    service = module.get<TasksService>(TasksService);
    repository = module.get<Repository<TaskEntity>>(
      getRepositoryToken(TaskEntity),
    );
  });

  afterEach(() => {
    jest.clearAllMocks();
  });

  describe('create', () => {
    it('should create a new task with QUEUED status', async () => {
      const createDto = {
        code: 'console.log("test")',
        prompt: 'Test task',
        timeout: 60,
      };

      const mockTask = {
        id: 'test-id',
        ...createDto,
        status: TaskStatus.QUEUED,
        createdAt: new Date(),
      } as TaskEntity;

      mockRepository.create.mockReturnValue(mockTask);
      mockRepository.save.mockResolvedValue(mockTask);

      const result = await service.create(createDto);

      expect(result).toEqual(mockTask);
      expect(mockRepository.create).toHaveBeenCalledWith({
        ...createDto,
        status: TaskStatus.QUEUED,
      });
      expect(mockRepository.save).toHaveBeenCalledWith(mockTask);
    });
  });

  describe('findOne', () => {
    it('should return a task when found', async () => {
      const mockTask = {
        id: 'test-id',
        status: TaskStatus.QUEUED,
      } as TaskEntity;

      mockRepository.findOne.mockResolvedValue(mockTask);

      const result = await service.findOne('test-id');

      expect(result).toEqual(mockTask);
      expect(mockRepository.findOne).toHaveBeenCalledWith({
        where: { id: 'test-id' },
      });
    });

    it('should throw NotFoundException when task not found', async () => {
      mockRepository.findOne.mockResolvedValue(null);

      await expect(service.findOne('non-existent')).rejects.toThrow(
        NotFoundException,
      );
    });
  });

  describe('update', () => {
    it('should update task status and set timestamps', async () => {
      const existingTask = {
        id: 'test-id',
        status: TaskStatus.QUEUED,
        createdAt: new Date(),
      } as TaskEntity;

      const updatedTask = {
        ...existingTask,
        status: TaskStatus.RUNNING,
        startedAt: new Date(),
      } as TaskEntity;

      mockRepository.findOne.mockResolvedValue(existingTask);
      mockRepository.save.mockResolvedValue(updatedTask);

      const result = await service.update('test-id', {
        status: TaskStatus.RUNNING,
      });

      expect(result.status).toBe(TaskStatus.RUNNING);
      expect(result.startedAt).toBeDefined();
    });

    it('should set completedAt when task is completed', async () => {
      const existingTask = {
        id: 'test-id',
        status: TaskStatus.RUNNING,
        startedAt: new Date(),
      } as TaskEntity;

      mockRepository.findOne.mockResolvedValue(existingTask);
      mockRepository.save.mockImplementation((task) => Promise.resolve(task));

      const result = await service.update('test-id', {
        status: TaskStatus.COMPLETED,
      });

      expect(result.completedAt).toBeDefined();
    });
  });

  describe('remove', () => {
    it('should delete completed task', async () => {
      const mockTask = {
        id: 'test-id',
        status: TaskStatus.COMPLETED,
      } as TaskEntity;

      mockRepository.findOne.mockResolvedValue(mockTask);
      mockRepository.remove.mockResolvedValue(mockTask);

      await service.remove('test-id');

      expect(mockRepository.remove).toHaveBeenCalledWith(mockTask);
    });

    it('should not allow deleting running task', async () => {
      const mockTask = {
        id: 'test-id',
        status: TaskStatus.RUNNING,
      } as TaskEntity;

      mockRepository.findOne.mockResolvedValue(mockTask);

      await expect(service.remove('test-id')).rejects.toThrow();
    });
  });

  describe('getStatistics', () => {
    it('should return task counts by status', async () => {
      mockRepository.count
        .mockResolvedValueOnce(100) // total
        .mockResolvedValueOnce(10) // queued
        .mockResolvedValueOnce(5) // running
        .mockResolvedValueOnce(80) // completed
        .mockResolvedValueOnce(5); // failed

      const result = await service.getStatistics();

      expect(result).toEqual({
        total: 100,
        queued: 10,
        running: 5,
        completed: 80,
        failed: 5,
      });
    });
  });
});
EOF

# Run tests
pnpm test
```

### Step 3: Write Integration Tests for API

```bash
# Create e2e test directory
mkdir -p test

# Configure e2e testing
cat > test/jest-e2e.json << 'EOF'
{
  "moduleFileExtensions": ["js", "json", "ts"],
  "rootDir": ".",
  "testEnvironment": "node",
  "testRegex": ".e2e-spec.ts$",
  "transform": {
    "^.+\\.(t|j)s$": "ts-jest"
  }
}
EOF

# Create tasks API e2e tests
cat > test/tasks.e2e-spec.ts << 'EOF'
import { Test, TestingModule } from '@nestjs/testing';
import { INestApplication, ValidationPipe } from '@nestjs/common';
import * as request from 'supertest';
import { AppModule } from './../src/app.module';

describe('Tasks API (e2e)', () => {
  let app: INestApplication;
  let createdTaskId: string;

  beforeAll(async () => {
    const moduleFixture: TestingModule = await Test.createTestingModule({
      imports: [AppModule],
    }).compile();

    app = moduleFixture.createNestApplication();

    // Apply same configuration as main.ts
    app.useGlobalPipes(
      new ValidationPipe({
        whitelist: true,
        forbidNonWhitelisted: true,
        transform: true,
      }),
    );

    await app.init();
  });

  afterAll(async () => {
    await app.close();
  });

  describe('/health (GET)', () => {
    it('should return healthy status', () => {
      return request(app.getHttpServer())
        .get('/health')
        .expect(200)
        .expect((res) => {
          expect(res.body.status).toBeDefined();
          expect(res.body.timestamp).toBeDefined();
        });
    });
  });

  describe('/tasks (POST)', () => {
    it('should create a new task', () => {
      return request(app.getHttpServer())
        .post('/tasks')
        .send({
          code: 'console.log("e2e test")',
          prompt: 'E2E test task',
          timeout: 60,
        })
        .expect(202)
        .expect((res) => {
          expect(res.body.id).toBeDefined();
          expect(res.body.status).toBe('queued');
          createdTaskId = res.body.id;
        });
    });

    it('should reject invalid task', () => {
      return request(app.getHttpServer())
        .post('/tasks')
        .send({
          code: '', // Empty code - should fail validation
          prompt: 'Invalid task',
        })
        .expect(400);
    });

    it('should reject task with timeout out of range', () => {
      return request(app.getHttpServer())
        .post('/tasks')
        .send({
          code: 'console.log("test")',
          prompt: 'Test',
          timeout: 5000, // Exceeds max
        })
        .expect(400);
    });
  });

  describe('/tasks (GET)', () => {
    it('should list tasks', () => {
      return request(app.getHttpServer())
        .get('/tasks')
        .expect(200)
        .expect((res) => {
          expect(res.body.tasks).toBeDefined();
          expect(Array.isArray(res.body.tasks)).toBe(true);
          expect(res.body.total).toBeGreaterThan(0);
        });
    });

    it('should filter tasks by status', () => {
      return request(app.getHttpServer())
        .get('/tasks?status=queued')
        .expect(200)
        .expect((res) => {
          expect(res.body.tasks).toBeDefined();
          // All returned tasks should have queued status
          res.body.tasks.forEach((task: any) => {
            expect(task.status).toBe('queued');
          });
        });
    });

    it('should paginate results', () => {
      return request(app.getHttpServer())
        .get('/tasks?limit=5&offset=0')
        .expect(200)
        .expect((res) => {
          expect(res.body.tasks.length).toBeLessThanOrEqual(5);
          expect(res.body.limit).toBe(5);
          expect(res.body.offset).toBe(0);
        });
    });
  });

  describe('/tasks/:id (GET)', () => {
    it('should get task by id', () => {
      return request(app.getHttpServer())
        .get(`/tasks/${createdTaskId}`)
        .expect(200)
        .expect((res) => {
          expect(res.body.id).toBe(createdTaskId);
          expect(res.body.status).toBeDefined();
        });
    });

    it('should return 404 for non-existent task', () => {
      return request(app.getHttpServer())
        .get('/tasks/00000000-0000-0000-0000-000000000000')
        .expect(404);
    });

    it('should return 400 for invalid UUID', () => {
      return request(app.getHttpServer())
        .get('/tasks/invalid-uuid')
        .expect(400);
    });
  });

  describe('/tasks/stats (GET)', () => {
    it('should return task statistics', () => {
      return request(app.getHttpServer())
        .get('/tasks/stats')
        .expect(200)
        .expect((res) => {
          expect(res.body.database).toBeDefined();
          expect(res.body.database.total).toBeGreaterThanOrEqual(0);
          expect(res.body.queue).toBeDefined();
        });
    });
  });
});
EOF

# Install supertest for HTTP testing
pnpm add -D supertest @types/supertest

# Run e2e tests
pnpm test:e2e
```

### Step 4: Write End-to-End System Tests

```bash
# Create system test script
cat > ../../scripts/test-system-e2e.sh << 'EOF'
#!/bin/bash

set -e

API_URL="${API_URL:-http://localhost:3000}"
TIMEOUT=60

echo "ðŸ§ª Running End-to-End System Tests"
echo "===================================="
echo "API URL: $API_URL"
echo ""

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Test counter
tests_passed=0
tests_failed=0

# Test function
test_case() {
  local name="$1"
  local command="$2"

  echo -n "Testing: $name... "

  if eval "$command" > /dev/null 2>&1; then
    echo -e "${GREEN}âœ“ PASS${NC}"
    ((tests_passed++))
  else
    echo -e "${RED}âœ— FAIL${NC}"
    ((tests_failed++))
  fi
}

# 1. Health Check
echo "1. Health Checks"
echo "----------------"
test_case "Orchestrator health" "curl -f $API_URL/health"
test_case "Database connected" "curl -s $API_URL/health | jq -e '.database.connected == true'"
test_case "Redis connected" "curl -s $API_URL/health | jq -e '.redis.connected == true'"
echo ""

# 2. Create Task
echo "2. Task Creation"
echo "----------------"
TASK_ID=$(curl -s -X POST $API_URL/tasks \
  -H "Content-Type: application/json" \
  -d '{
    "code": "console.log(\"E2E test: \" + (2 + 2))",
    "prompt": "E2E system test",
    "timeout": 30
  }' | jq -r '.id')

test_case "Task created" "[[ ! -z '$TASK_ID' && '$TASK_ID' != 'null' ]]"
echo "Task ID: $TASK_ID"
echo ""

# 3. Wait for Completion
echo "3. Task Execution"
echo "-----------------"
echo "Waiting for task completion (timeout: ${TIMEOUT}s)..."

start_time=$(date +%s)
completed=false

while [ $(($(date +%s) - start_time)) -lt $TIMEOUT ]; do
  status=$(curl -s $API_URL/tasks/$TASK_ID | jq -r '.status')
  echo -n "."

  if [ "$status" == "completed" ]; then
    completed=true
    break
  elif [ "$status" == "failed" ]; then
    echo -e "\n${RED}Task failed${NC}"
    curl -s $API_URL/tasks/$TASK_ID | jq .
    exit 1
  fi

  sleep 2
done

echo ""

if [ "$completed" = true ]; then
  echo -e "${GREEN}âœ“ Task completed${NC}"
  ((tests_passed++))
else
  echo -e "${RED}âœ— Task did not complete within timeout${NC}"
  ((tests_failed++))
fi

# 4. Verify Result
echo ""
echo "4. Result Verification"
echo "----------------------"
result=$(curl -s $API_URL/tasks/$TASK_ID)

test_case "Task status is completed" "echo '$result' | jq -e '.status == \"completed\"'"
test_case "Result contains stdout" "echo '$result' | jq -e '.result.stdout != null'"
test_case "Exit code is 0" "echo '$result' | jq -e '.result.exitCode == 0'"
test_case "Worker ID recorded" "echo '$result' | jq -e '.workerId != null'"

echo ""
echo "Task result:"
echo "$result" | jq '.result'
echo ""

# 5. Statistics
echo "5. System Statistics"
echo "--------------------"
stats=$(curl -s $API_URL/tasks/stats)

echo "Database stats:"
echo "$stats" | jq '.database'

echo ""
echo "Queue stats:"
echo "$stats" | jq '.queue'

test_case "Total tasks > 0" "echo '$stats' | jq -e '.database.total > 0'"
test_case "Completed tasks > 0" "echo '$stats' | jq -e '.database.completed > 0'"
echo ""

# 6. Worker Health
echo "6. Worker Health"
echo "----------------"
workers=$(curl -s $API_URL/workers)
worker_count=$(echo "$workers" | jq '.workers | length')

echo "Active workers: $worker_count"
test_case "At least 1 worker active" "[[ $worker_count -gt 0 ]]"

if [ $worker_count -gt 0 ]; then
  echo ""
  echo "Worker details:"
  echo "$workers" | jq '.workers[]'
fi

echo ""

# Summary
echo "===================================="
echo "ðŸ“Š Test Summary"
echo "===================================="
echo -e "Tests passed: ${GREEN}$tests_passed${NC}"
echo -e "Tests failed: ${RED}$tests_failed${NC}"
echo ""

if [ $tests_failed -eq 0 ]; then
  echo -e "${GREEN}âœ… All tests passed!${NC}"
  exit 0
else
  echo -e "${RED}âŒ Some tests failed${NC}"
  exit 1
fi
EOF

chmod +x ../../scripts/test-system-e2e.sh

# Run system tests
../../scripts/test-system-e2e.sh
```

### Step 5: Add Test Scripts to package.json

```bash
# Update orchestrator package.json
cat >> package.json << 'EOF'
{
  "scripts": {
    "test": "jest",
    "test:watch": "jest --watch",
    "test:cov": "jest --coverage",
    "test:debug": "node --inspect-brk -r tsconfig-paths/register -r ts-node/register node_modules/.bin/jest --runInBand",
    "test:e2e": "jest --config ./test/jest-e2e.json"
  }
}
EOF
```

### Step 6: Generate Test Coverage Report

```bash
# Run tests with coverage
pnpm test:cov

# View coverage report
open coverage/lcov-report/index.html

# Or view in terminal
cat coverage/coverage-summary.json
```

### Step 7: Create Final README

```bash
cat > ../../README.md << 'EOF'
# Claude Orchestrator

A distributed task orchestration system for Claude Code workers built with NestJS, TypeScript, BullMQ, and Redis.

![Architecture](docs/architecture-diagram.png)

## ðŸŒŸ Features

- âœ… **Distributed Task Execution** - Run tasks on cloud or local workers
- âœ… **Reliable Job Queue** - BullMQ with automatic retries and error handling
- âœ… **Real-time Monitoring** - Web dashboard with live statistics
- âœ… **Type-Safe** - Full TypeScript implementation
- âœ… **Scalable** - Horizontal scaling of workers
- âœ… **Production-Ready** - Docker deployment, health checks, logging

## ðŸ“‹ Table of Contents

- [Quick Start](#quick-start)
- [Architecture](#architecture)
- [Installation](#installation)
- [Usage](#usage)
- [Development](#development)
- [Testing](#testing)
- [Deployment](#deployment)
- [API Documentation](#api-documentation)
- [Contributing](#contributing)

## ðŸš€ Quick Start

### Prerequisites

- Node.js 20+
- pnpm 8+
- Docker & Docker Compose
- Redis

### Installation

```bash
# Clone repository
git clone https://github.com/your-username/claude-orchestrator.git
cd claude-orchestrator

# Run setup
./scripts/setup.sh

# Configure environment
cp .env.example .env
nano .env  # Edit configuration

# Start services
docker compose up redis -d

# Start orchestrator
cd apps/orchestrator
pnpm dev

# Start worker
cd apps/worker
pnpm dev
```

### Create Your First Task

```bash
curl -X POST http://localhost:3000/tasks \
  -H "Content-Type: application/json" \
  -d '{
    "code": "console.log(\"Hello, World!\")",
    "prompt": "Print hello world"
  }'
```

## ðŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   REST API  â”‚ â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Orchestrator â”‚ â”€â”€â”€â”€â”€â”€â”€â–¶â”‚    Redis     â”‚
â”‚  (FastAPI)  â”‚         â”‚   (NestJS)   â”‚         â”‚   (BullMQ)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚                          â”‚
                               â”‚                          â”‚
                               â–¼                          â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   Database   â”‚         â”‚   Workers    â”‚
                        â”‚  (TypeORM)   â”‚         â”‚  (Multiple)  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

See [docs/architecture-overview.md](docs/architecture-overview.md) for detailed architecture.

## ðŸ“¦ Project Structure

```
claude-orchestrator/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ orchestrator/    # NestJS API service
â”‚   â”œâ”€â”€ worker/          # Task execution worker
â”‚   â””â”€â”€ dashboard/       # Web monitoring dashboard
â”œâ”€â”€ packages/
â”‚   â””â”€â”€ shared/          # Shared TypeScript types
â”œâ”€â”€ docs/                # Documentation
â”œâ”€â”€ tasks/               # Implementation guides
â””â”€â”€ scripts/             # Utility scripts
```

## ðŸ’» Development

### Running Locally

```bash
# Terminal 1: Start Redis
docker compose up redis -d

# Terminal 2: Start orchestrator
cd apps/orchestrator
pnpm dev

# Terminal 3: Start worker
cd apps/worker
pnpm dev

# Terminal 4: Start dashboard
cd apps/dashboard
pnpm dev
```

### Running Tests

```bash
# Unit tests
pnpm test

# E2E tests
pnpm test:e2e

# Coverage
pnpm test:cov

# System tests
./scripts/test-system-e2e.sh
```

### Code Quality

```bash
# Lint
pnpm lint

# Format
pnpm format

# Type check
pnpm build
```

## ðŸ§ª Testing

The project includes comprehensive testing:

- **Unit Tests** - Jest tests for services and utilities
- **Integration Tests** - API endpoint testing
- **E2E Tests** - Complete system flow testing

Test coverage: **85%+**

Run all tests:
```bash
pnpm test:all
```

## ðŸš€ Deployment

### Docker

```bash
# Build images
docker-compose build

# Start all services
docker-compose --profile production up -d
```

### Cloud Platforms

Deployment guides available for:
- [DigitalOcean](tasks/task-06-deploy-cloud.md#option-a-digitalocean-deployment)
- [AWS](tasks/task-06-deploy-cloud.md#option-b-aws-deployment)
- [fly.io](tasks/task-06-deploy-cloud.md#option-c-flyio-deployment)

See [tasks/task-06-deploy-cloud.md](tasks/task-06-deploy-cloud.md) for full deployment guide.

## ðŸ“š API Documentation

Interactive API documentation available at:
```
http://localhost:3000/api
```

Key endpoints:
- `POST /tasks` - Create task
- `GET /tasks` - List tasks
- `GET /tasks/:id` - Get task details
- `GET /tasks/stats` - Get statistics
- `GET /health` - Health check

Full API reference: [docs/api-reference.md](docs/api-reference.md)

## ðŸ”§ Configuration

Key environment variables:

```bash
# Redis
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=your_password

# Database
DB_TYPE=sqlite  # or postgres
DB_DATABASE=./data/tasks.db

# Orchestrator
PORT=3000
NODE_ENV=development

# Worker
WORKER_ID=worker-1
WORKER_TYPE=local
```

See [.env.example](.env.example) for all options.

## ðŸ“– Documentation

- [Getting Started](docs/nestjs-getting-started.md)
- [Architecture Overview](docs/architecture-overview.md)
- [Tech Stack](docs/tech-stack.md)
- [API Reference](docs/api-reference.md)
- [Development Guide](docs/development-guide.md)
- [Implementation Tasks](tasks/)

## ðŸ¤ Contributing

Contributions welcome! Please read [CONTRIBUTING.md](CONTRIBUTING.md) first.

### Development Workflow

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Run tests and linting
6. Submit a pull request

## ðŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

## ðŸ™ Acknowledgments

- Built with [NestJS](https://nestjs.com/)
- Queue management by [BullMQ](https://docs.bullmq.io/)
- Database ORM by [TypeORM](https://typeorm.io/)

## ðŸ“ž Support

- **Documentation**: [docs/](docs/)
- **Issues**: [GitHub Issues](https://github.com/your-username/claude-orchestrator/issues)
- **Discussions**: [GitHub Discussions](https://github.com/your-username/claude-orchestrator/discussions)

## ðŸ—º Roadmap

- [ ] GraphQL API
- [ ] WebSocket support for real-time updates
- [ ] Advanced task prioritization
- [ ] Task dependencies and workflows
- [ ] Multi-language code execution (not just JavaScript)
- [ ] Built-in metrics and alerting
- [ ] Admin panel for task management

---

Made with â¤ï¸ by the Claude Orchestrator team
EOF
```

### Step 8: Create Contributing Guide

```bash
cat > ../../CONTRIBUTING.md << 'EOF'
# Contributing to Claude Orchestrator

Thank you for your interest in contributing! This document provides guidelines for contributing to the project.

## Code of Conduct

Please be respectful and constructive in all interactions.

## How to Contribute

### Reporting Bugs

1. Check if the bug has already been reported
2. Use the bug report template
3. Include reproduction steps
4. Include environment details

### Suggesting Features

1. Check if the feature has been suggested
2. Use the feature request template
3. Explain the use case
4. Provide examples if possible

### Pull Requests

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/my-feature`
3. Make your changes
4. Add tests for new functionality
5. Ensure all tests pass: `pnpm test`
6. Lint your code: `pnpm lint`
7. Commit with clear messages
8. Push to your fork
9. Open a pull request

### Commit Message Format

```
type(scope): subject

body

footer
```

Types: feat, fix, docs, style, refactor, test, chore

Example:
```
feat(worker): add support for Python code execution

- Implement Python executor
- Add sandbox environment
- Update tests

Closes #123
```

## Development Setup

See [docs/development-guide.md](docs/development-guide.md)

## Testing Guidelines

- Write unit tests for all new services
- Write integration tests for API endpoints
- Maintain 80%+ code coverage
- Test edge cases and error conditions

## Documentation

- Update relevant documentation
- Add JSDoc comments to public APIs
- Update README if needed
- Add examples for new features

## Questions?

Open a discussion on GitHub or reach out to maintainers.

Thank you for contributing! ðŸŽ‰
EOF
```

## Verification Checklist

- [ ] All unit tests pass
- [ ] Integration tests pass
- [ ] E2E system tests pass
- [ ] Test coverage >80%
- [ ] README.md complete
- [ ] API documentation complete
- [ ] CONTRIBUTING.md created
- [ ] All documentation reviewed
- [ ] License file added

## Running All Tests

```bash
# Create master test script
cat > ../../scripts/test-all.sh << 'EOF'
#!/bin/bash

set -e

echo "ðŸ§ª Running All Tests"
echo "===================="
echo ""

# 1. Unit tests
echo "1. Running unit tests..."
cd apps/orchestrator
pnpm test
cd ../..
echo "âœ… Unit tests passed"
echo ""

# 2. E2E API tests
echo "2. Running E2E API tests..."
cd apps/orchestrator
pnpm test:e2e
cd ../..
echo "âœ… E2E tests passed"
echo ""

# 3. System E2E tests
echo "3. Running system E2E tests..."
./scripts/test-system-e2e.sh
echo "âœ… System tests passed"
echo ""

# 4. Generate coverage report
echo "4. Generating coverage report..."
cd apps/orchestrator
pnpm test:cov
echo "âœ… Coverage report generated"
echo "   View at: apps/orchestrator/coverage/lcov-report/index.html"
cd ../..
echo ""

echo "===================================="
echo "âœ… All tests passed successfully!"
echo "===================================="
EOF

chmod +x ../../scripts/test-all.sh

# Run all tests
../../scripts/test-all.sh
```

## Final Documentation Review

### Checklist

- [ ] All docs in docs/ folder reviewed and complete
- [ ] All tasks in tasks/ folder tested and accurate
- [ ] README.md includes all essential information
- [ ] API documentation matches actual endpoints
- [ ] Architecture diagrams accurate
- [ ] Environment variable documentation complete
- [ ] Deployment guides tested
- [ ] Troubleshooting sections helpful

## Next Steps

ðŸŽ‰ **Congratulations!** You have completed all implementation tasks!

Your Claude Orchestrator system is now:
- âœ… Fully implemented
- âœ… Comprehensively tested
- âœ… Thoroughly documented
- âœ… Production-ready

### What's Next?

1. **Deploy to production** - Follow Task 06 deployment guide
2. **Monitor and maintain** - Set up alerts and monitoring
3. **Gather feedback** - Use in real projects
4. **Contribute improvements** - Share enhancements with the community
5. **Extend functionality** - Add features from the roadmap

### Support

- Read the docs: [docs/](../docs/)
- Check examples: [scripts/](../scripts/)
- Ask questions: GitHub Discussions
- Report issues: GitHub Issues

Thank you for building Claude Orchestrator! ðŸš€